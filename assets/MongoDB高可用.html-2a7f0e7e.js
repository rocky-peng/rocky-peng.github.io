import{_ as d,$ as c,a0 as E,a1 as i,a2 as l,a3 as t,a4 as e,a5 as n,w as o}from"./framework-58787b1c.js";const s={},u=n('<h2 id="单节点" tabindex="-1"><a class="header-anchor" href="#单节点" aria-hidden="true">#</a> 单节点</h2><p>分析：</p><ol><li>由于单节点，读写均在一个节点上进行，读写压力可能比较大</li><li>存在单点故障问题，一旦出现问题，整个服务都不用</li><li>存储的数据量也会成为一个瓶颈</li></ol><p>优点：</p><ol><li>结构简单，已部署，已维护，客户端使用起来也方便</li></ol><h2 id="主从架构" tabindex="-1"><a class="header-anchor" href="#主从架构" aria-hidden="true">#</a> 主从架构</h2><figure><img src="https://cdn.justdopay.com/notion/md5-7ab18e5340f9abb9d632acaa29767ddc.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><p>特点：</p><ol><li>写数据只能在master上进行</li><li>slave之间互相不能感知</li></ol><p>分析：</p><ol><li>存在主从必定存在数据复制的过程，那么逃不了要面临数据同步延迟问题，也就是数据不一致问题</li><li>如果master宕机后正好数据同步不及时，那么数据就会丢失</li><li>由于slave之间不能互相感知，并且又没有另外的类似监控角色存在，所以一旦master宕机，是没办法自动故障转移的，必须人为参与，那么SLA（就是指几个9，或者可用性）必定大打折扣</li></ol><p>从MongoDB 3.6 开始已经彻底废弃不使用了</p><h2 id="副本集架构" tabindex="-1"><a class="header-anchor" href="#副本集架构" aria-hidden="true">#</a> 副本集架构</h2><ol><li>有三种角色：Primary，Secondary，Arbiter（可选），下面可能会描述为主节点（Primary）、从节点（Secondary）</li><li>Arbiter不储存数据，只负责投票，并且是可选配置</li><li>Primary节点有且仅有一个</li><li>在某些特定场合（比如网络出现分区），Primary节点可能出现多个，但通常只有最新的primary节点能正常响应写请求</li><li>Primary会把对数据的所有改动都记录在oplog日志中</li><li>从节点会同步primary节点的oplog，然后根据oplog进行重放，从而保持与Primary数据同步</li><li>如果某个时候Primary节点不可用，那么从节点会发起选举要求自己成为主节点</li><li>在某个情况下（比如成本限制）没有条件部署多余的从节点，此时可以选择部署一个Arbiter，Arbiter只复制选举，不负责数据存储。通过这样的方式既能节约成本，又能保证高可用。比如注意观察下面的图</li><li>当主节点生产oplog的速率大于从节点复制oplog的速率，那么数据延迟会逐渐增长。当超过一个阈值后，主节点会开启流量控制。</li><li>主节点流量控制：一言蔽之就是主节点会限制应用写入数据的速度。如何限制呢？ 当主节点开启流控后，应用写入数据前需先请求一个票据（可以理解为token），主节点通过限制票据的生产速度来达到流控的目的</li><li>当从节点与主节点断开联系超过一定的时间，那么符合条件的从节点会发起选举要求自己成为主节点</li><li>副本集群在选主过程中，是不能对外提供写服务的（这是一种不可用的表现），但保留了提供读服务的能力。（用不用取决于客户端，客户端可以配置在主节点脱机时是否在从节点上执行）</li><li>默认情况下，客户端的读操作由主节点完成。但客户端可以通过配置从而让从节点来完成读操作</li><li>下面的图全部来自于mongodb官网</li><li></li></ol><figure><img src="https://cdn.justdopay.com/notion/md5-79738901ccb86d2bf9f2dc15c4d50bc0.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><figure><img src="https://cdn.justdopay.com/notion/md5-6aafe4968edf1bd7b5a71787a70913a2.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><figure><img src="https://cdn.justdopay.com/notion/md5-f432036a4148ea95e5863f4eb23fa333.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><figure><img src="https://cdn.justdopay.com/notion/md5-98caa44cd14c49a05cc9b6c13d261132.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><figure><img src="https://cdn.justdopay.com/notion/md5-d0250def39e583c4b260257040ad55d7.png" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><p>本质也是主从架构，只是增加了第三个角色：仲裁者（Arbiter），然后更换了角色名以及职责</p>',20),f={href:"https://www.mongodb.com/docs/manual/replication/",target:"_blank",rel:"noopener noreferrer"},p=n('<p>特点：</p><ol><li>primary节点负责读写请求的处理（客户端可以指定把读请求交给副本节点完成）</li><li>整个集群中，各个节点之间有心跳信息，也就是说从节点间能互相感知</li><li>primary节点宕机后，会从seondary节点中选择一个作为新的primay节点对外提供服务</li><li>arbiter节点不存储数据，只进行选主投票</li></ol><p>分析：</p><ol><li>通过增加仲裁者的角色以及节点间互相感知的功能，可以做到故障自动转移</li><li>仍然存在数据不一致或丢失的问题</li></ol><br><h2 id="系统推荐" tabindex="-1"><a class="header-anchor" href="#系统推荐" aria-hidden="true">#</a> 系统推荐</h2>',6),h=i("p",null,[i("br"),i("br"),i("br"),i("br"),i("br"),i("br")],-1),g=i("hr",null,null,-1),m=i("hr",null,null,-1),B=i("ul",null,[i("li",null,[i("strong",null,"随机毒鸡汤"),l("：哪里会有人喜欢孤独，不过是，不喜欢失望罢了。 "),i("br"),i("br"),i("img",{src:"https://tuapi.eees.cc/api.php?category=fengjing&type=302&uuid=8d06f312-0968-4928-9afb-d6b033ff32ad",alt:"",loading:"lazy"})])],-1);function A(b,_){const r=o("ExternalLinkIcon"),a=o("RouterLink");return c(),E("div",null,[u,i("p",null,[l("扩展阅读："),i("a",f,[l("https://www.mongodb.com/docs/manual/replication/"),t(r)])]),p,i("ul",null,[i("li",null,[t(a,{to:"/software/unclassified/Oh%20My%20ZSH.html"},{default:e(()=>[l("Oh My ZSH")]),_:1})]),i("li",null,[t(a,{to:"/software/java-basic/synchronized%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90.html"},{default:e(()=>[l("synchronized原理深度剖析")]),_:1})]),i("li",null,[t(a,{to:"/software/unclassified/%E7%BA%BF%E4%B8%8AFullGC%E9%A2%91%E7%B9%81%E7%9A%84%E6%8E%92%E6%9F%A5.html"},{default:e(()=>[l("线上FullGC频繁的排查")]),_:1})]),i("li",null,[t(a,{to:"/software/unclassified/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9git%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95%E4%B8%AD%E7%9A%84%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E9%82%AE%E7%AE%B1.html"},{default:e(()=>[l("批量修改git历史记录中的用户名和邮箱")]),_:1})]),i("li",null,[t(a,{to:"/software/java-basic/%E5%B8%B8%E7%94%A8%E9%98%9F%E5%88%97.html"},{default:e(()=>[l("常用队列")]),_:1})]),i("li",null,[t(a,{to:"/software/unclassified/%E5%88%B6%E4%BD%9CKVM%20ES%E9%95%9C%E5%83%8F%E6%96%87%E4%BB%B6.html"},{default:e(()=>[l("制作KVM ES镜像文件")]),_:1})]),i("li",null,[t(a,{to:"/other/Git%20Merge%20%E3%80%81Rebase.html"},{default:e(()=>[l("Git Merge 、Rebase")]),_:1})]),i("li",null,[t(a,{to:"/software/unclassified/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%8A%80%E6%9C%AF.html"},{default:e(()=>[l("前后端常用技术")]),_:1})]),i("li",null,[t(a,{to:"/other/Nginx%E7%9A%84%E5%8F%8C%E5%90%91%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE.html"},{default:e(()=>[l("Nginx的双向认证配置")]),_:1})]),i("li",null,[t(a,{to:"/other/Git%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95%E4%BF%AE%E6%94%B9%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E9%82%AE%E7%AE%B1.html"},{default:e(()=>[l("Git历史记录修改用户名和邮箱")]),_:1})]),i("li",null,[t(a,{to:"/software/docker/Docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1%E6%96%B9%E6%A1%88.html"},{default:e(()=>[l("Docker跨主机通信方案")]),_:1})]),i("li",null,[t(a,{to:"/other/Docker%E9%9A%90%E5%B0%84%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%A4%96%E7%BD%91%E4%B8%8D%E8%83%BD%E8%AE%BF%E9%97%AE.html"},{default:e(()=>[l("Docker隐射的端口外网不能访问")]),_:1})])]),h,g,m,B])}const F=d(s,[["render",A],["__file","MongoDB高可用.html.vue"]]);export{F as default};
